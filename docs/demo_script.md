# Rail-CRAG 演示脚本

## 🎬 开场 (0:00 - 0:30)
- **画面**: VS Code 打开 README.md 架构图。
- **解说**: "大家好，这是 Rail-CRAG。这是一个基于 CRAG 论文实现的铁路标准问答 Agent。针对传统 RAG 容易产生幻觉的问题，我们引入了检索评估器和纠错机制。"
- **操作**: 切换到终端，运行 docker-compose up，展示系统启动过程。

## 🔍 第一部分：AI 辅助开发展示 (0:30 - 1:30)
- **画面**: 打开 src/components/evaluator.py。
- **解说**: "在开发核心的评估器时，我使用了 AI Agent 辅助编程。CRAG 要求对文档进行严格打分，我通过 Cursor 让 AI 使用 Pydantic 来保证 LLM 输出的结构化安全，避免了运行时错误。"
- **操作**: 选中一段代码，展示 Git Lens 的提交记录或重现一次 AI 生成代码的过程。

## 💡 第二部分：CRAG 核心功能演示 (1:30 - 3:30)
- **画面**: 浏览器打开 Streamlit 界面 (localhost:8501)。

### 场景 1：精准检索 (Correct)
- **输入**: "标准轨距是多少？"
- **解说**: "当问题在知识库中有明确答案时..."
- **展示**: 侧边栏状态显示 🟢 **CORRECT**。
- [cite_start]**亮点**: 展开 "Knowledge Refinement" 折叠面板，说明："大家看，系统自动过滤了文档中的无关段落，只保留了关于 1435mm 的核心定义。" [cite: 11]

### 场景 2：触发搜索 (Incorrect -> Correction)
- **输入**: "2050年火星铁路的建设标准是什么？"
- **解说**: "当遇到知识库中不存在的幻觉陷阱时..."
- **展示**: 侧边栏状态显示 🔴 **INCORRECT**。
- [cite_start]**亮点**: 指出系统触发了 **Query Rewrite** [cite: 10]，将问题重写为关键词，并调用了 Google/Tavily 搜索，最终回答 "目前没有相关标准"，而不是胡编乱造。

## 📊 结尾 (3:30 - 4:00)
- **画面**: 终端运行 python -m src.evaluation.benchmark_comparison 的结果表格。
- **解说**: "通过基准测试，我们发现 CRAG 在处理模糊问题时的准确率明显高于普通 RAG。以上就是演示，谢谢。"
